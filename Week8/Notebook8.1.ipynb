{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Walidsati/AAI_612O/blob/main/Week8/Notebook8.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUDaVUCyp2t7"
      },
      "source": [
        "\n",
        "# AAI612: Deep Learning & its Applications\n",
        "\n",
        "*Notebook 8.1: Text Preprocessing Using *Scikit-learn*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Programming Transformers with Keras\n",
        "#### Adopted from Jeff Heaton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14QUyFpKp2uA"
      },
      "source": [
        "This Notebook shows an example of a transformer encoder to predict sunspots.  You can find the data files needed for this example at the following location.\n",
        "\n",
        "* [Sunspot Data Files](http://www.sidc.be/silso/datafiles#total)\n",
        "* [Download Daily Sunspots](http://www.sidc.be/silso/INFO/sndtotcsv.php) - 1/1/1818 to now.\n",
        "\n",
        "The following code loads the sunspot file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u12-bjaOp2uA",
        "outputId": "87ba75e5-68b2-4e10-d8a2-58667e630de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting file:\n",
            "   year  month  day  dec_year  sn_value  sn_error  obs_num  extra\n",
            "0  1818      1    1  1818.001        -1       NaN        0      1\n",
            "1  1818      1    2  1818.004        -1       NaN        0      1\n",
            "2  1818      1    3  1818.007        -1       NaN        0      1\n",
            "3  1818      1    4  1818.010        -1       NaN        0      1\n",
            "4  1818      1    5  1818.012        -1       NaN        0      1\n",
            "5  1818      1    6  1818.015        -1       NaN        0      1\n",
            "6  1818      1    7  1818.018        -1       NaN        0      1\n",
            "7  1818      1    8  1818.021        65      10.2        1      1\n",
            "8  1818      1    9  1818.023        -1       NaN        0      1\n",
            "9  1818      1   10  1818.026        -1       NaN        0      1\n",
            "Ending file:\n",
            "       year  month  day  dec_year  sn_value  sn_error  obs_num  extra\n",
            "75077  2023      7   22  2023.555       139      16.1       36      0\n",
            "75078  2023      7   23  2023.558       141      17.6       29      0\n",
            "75079  2023      7   24  2023.560       137      14.9       31      0\n",
            "75080  2023      7   25  2023.563       136      12.9       35      0\n",
            "75081  2023      7   26  2023.566       153      15.2       33      0\n",
            "75082  2023      7   27  2023.568       145      15.1       18      0\n",
            "75083  2023      7   28  2023.571       164      18.6       27      0\n",
            "75084  2023      7   29  2023.574       179      17.9       33      0\n",
            "75085  2023      7   30  2023.577       180      16.4       36      0\n",
            "75086  2023      7   31  2023.579       183      21.3       19      0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import ssl\n",
        "\n",
        "names = ['year', 'month', 'day', 'dec_year', 'sn_value' ,\n",
        "         'sn_error', 'obs_num', 'extra']\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/harmanani/AAI612/main/Week8/SN_d_tot_V2.0.csv\",\n",
        "    sep=';',header=None,names=names,\n",
        "    na_values=['-1'], index_col=False)\n",
        "\n",
        "print(\"Starting file:\")\n",
        "print(df[0:10])\n",
        "\n",
        "print(\"Ending file:\")\n",
        "print(df[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzV1XGS0p2uA"
      },
      "source": [
        "As you can see, there is quite a bit of missing data near the end of the file.  We want to find the starting index where the missing data no longer occurs.  This technique is somewhat sloppy; it would be better to find a use for the data between missing values.  However, the point of this example is to show how to use a transformer encoder with a somewhat simple time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2pQeFAzp2uA",
        "outputId": "ebabe1d2-14f5-4b57-e4fa-f4af960518ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314\n"
          ]
        }
      ],
      "source": [
        "# Find the last zero and move one beyond\n",
        "start_id = max(df[df['obs_num'] == 0].index.tolist())+1\n",
        "print(start_id)\n",
        "df = df[start_id:] # Trim the rows that have missing observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNZcj-s6Vp2R"
      },
      "source": [
        "Divide into training and test/validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBaewsY_p2uB",
        "outputId": "73624a88-6bd5-45f6-fcd9-c74908a60608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 55160 observations.\n",
            "Test set has 8613 observations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d1eda0cf03a9>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['sn_value'] = df['sn_value'].astype(float)\n"
          ]
        }
      ],
      "source": [
        "df['sn_value'] = df['sn_value'].astype(float)\n",
        "df_train = df[df['year']<2000]\n",
        "df_test = df[df['year']>=2000]\n",
        "\n",
        "spots_train = df_train['sn_value'].tolist()\n",
        "spots_test = df_test['sn_value'].tolist()\n",
        "\n",
        "print(\"Training set has {} observations.\".format(len(spots_train)))\n",
        "print(\"Test set has {} observations.\".format(len(spots_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm9P7RnqYQzh"
      },
      "source": [
        "The **to_sequences** function takes linear time series data into an **x** and **y** where **x** is all possible sequences of seq_size. After each **x** sequence, this function places the next value into the **y** variable. These **x** and **y** data can train a time-series neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaltDADPp2uB",
        "outputId": "c4c984d6-818c-4578-eb68-3ad0ddb77d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training set: (55150, 10, 1)\n",
            "Shape of test set: (8603, 10, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_sequences(seq_size, obs):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(obs)-SEQUENCE_SIZE):\n",
        "        #print(i)\n",
        "        window = obs[i:(i+SEQUENCE_SIZE)]\n",
        "        after_window = obs[i+SEQUENCE_SIZE]\n",
        "        window = [[x] for x in window]\n",
        "        #print(\"{} - {}\".format(window,after_window))\n",
        "        x.append(window)\n",
        "        y.append(after_window)\n",
        "\n",
        "    return np.array(x),np.array(y)\n",
        "\n",
        "\n",
        "SEQUENCE_SIZE = 10\n",
        "x_train,y_train = to_sequences(SEQUENCE_SIZE,spots_train)\n",
        "x_test,y_test = to_sequences(SEQUENCE_SIZE,spots_test)\n",
        "\n",
        "print(\"Shape of training set: {}\".format(x_train.shape))\n",
        "print(\"Shape of test set: {}\".format(x_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDjLR0RjYl5y"
      },
      "source": [
        "We can view the results of the **to_sequences** encoding of the sunspot data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNH3BG-jp2uB",
        "outputId": "7efe181a-4bc5-4818-b98f-a9d549ed1ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(55150, 10, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlmXgzbcZEHn"
      },
      "source": [
        "Next, we create the transformer_encoder; I obtained this function from a [Keras example](https://keras.io/examples/timeseries/timeseries_transformer_classification/). This layer includes residual connections, layer normalization, and dropout. This resulting layer can be stacked multiple times. We implement the projection layers with the Keras Conv1D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YZ5ltq1L397v"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et2peKf3Z4H_"
      },
      "source": [
        "The following function is provided to build the model, including the attention layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HUAVPayh4MKc"
      },
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkxS7ZpXc_Tf"
      },
      "source": [
        "We are now ready to build and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzacEsJip2uB",
        "outputId": "a1f67cc3-7896-4336-cce5-7a0f90d727d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 27ms/step - loss: 4999.2930 - val_loss: 484.8763\n",
            "Epoch 2/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 1420.0132 - val_loss: 373.6129\n",
            "Epoch 3/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 1065.3472 - val_loss: 370.0396\n",
            "Epoch 4/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 879.8105 - val_loss: 318.7425\n",
            "Epoch 5/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 796.7570 - val_loss: 318.9029\n",
            "Epoch 6/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 782.6698 - val_loss: 303.0057\n",
            "Epoch 7/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 737.4709 - val_loss: 314.2691\n",
            "Epoch 8/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 737.0095 - val_loss: 298.9435\n",
            "Epoch 9/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 728.2636 - val_loss: 297.2794\n",
            "Epoch 10/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 708.2012 - val_loss: 315.9437\n",
            "Epoch 11/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 709.2661 - val_loss: 298.8123\n",
            "Epoch 12/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 714.0947 - val_loss: 305.8804\n",
            "Epoch 13/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 694.5236 - val_loss: 303.6474\n",
            "Epoch 14/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 701.2787 - val_loss: 296.6670\n",
            "Epoch 15/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 703.9254 - val_loss: 290.3349\n",
            "Epoch 16/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 700.4928 - val_loss: 295.1387\n",
            "Epoch 17/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 694.9083 - val_loss: 298.6722\n",
            "Epoch 18/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 697.4297 - val_loss: 304.5894\n",
            "Epoch 19/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 699.9594 - val_loss: 291.7059\n",
            "Epoch 20/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 695.4466 - val_loss: 289.3759\n",
            "Epoch 21/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 681.0680 - val_loss: 300.9746\n",
            "Epoch 22/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 682.0863 - val_loss: 306.2912\n",
            "Epoch 23/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 691.3228 - val_loss: 300.1401\n",
            "Epoch 24/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 691.8675 - val_loss: 298.7050\n",
            "Epoch 25/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 671.0859 - val_loss: 295.8602\n",
            "Epoch 26/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 678.7462 - val_loss: 294.1782\n",
            "Epoch 27/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 685.3516 - val_loss: 291.9077\n",
            "Epoch 28/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 666.4945 - val_loss: 305.3780\n",
            "Epoch 29/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 650.6153 - val_loss: 292.2318\n",
            "Epoch 30/200\n",
            "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 677.6613 - val_loss: 310.4962\n",
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 280.4083\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "186.35598754882812"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"mean_squared_error\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "#model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, \\\n",
        "    restore_best_weights=True)]\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jeJf5zsp2uB"
      },
      "source": [
        "Finally, we evaluate the model with RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqiV9Oq2p2uB",
        "outputId": "57f5430f-c749-4c45-fe78-94fb6bd7bd9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n",
            "Score (RMSE): 13.651227108888328\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(\"Score (RMSE): {}\".format(score))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sW8HeXATtfat"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "t81_558_class_10_5_keras_transformers.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}